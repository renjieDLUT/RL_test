Hello, Reinforcement Learning;以交互目标为导向进行学习
"试探exploriing"和"开发exploitation"之间的折中权衡.
# 导论
## RL要素

# 重要概念
1. **自举**: 利用已有的估计值(状态价值,动作价值)来更新当前的估计值的过程.时序差分TD是一种自举.
2. **利用(exploitation)**与**试探(exploration)**
3. **优化控制**,**试错理论**,**时序差分**:
4. **行动器-评判器AC(actor-critic)**: 试错学习中使用时序差分学习
5. **时序差分**:价值变化更新的部分依赖于两个不同时刻的状态的价值的差.
6. **开环(open-loop)**与**闭环(closed-loop)**:主要区别在于系统是否根据环境或输出的反馈来调整其输入或行为.
7. **Q-learning**:是一种基于模型的强化学习技术,利用环境模型(状态转移概率和奖励函数)来生成模拟数据,进而通过模拟数据来更新动作价值函数的方法.(**模拟数据生成**,**价值函数更新**)
8. **Dyna-Q**:agent在模型生成的模拟环境中进行规划和学习,同时也在真实环境中进行交互和学习.